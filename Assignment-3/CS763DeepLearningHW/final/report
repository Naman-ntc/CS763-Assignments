We trained several models using ReLU, liner (fully connected) and batchNorm (both parametric and non-parametric). We also used sgd with momentum

We tried the following models (in the order in which we tried them):


No		Model 														trainAcc 			valAcc		Loss

1		3 hidden layers with BN (0.2)								58					49			1(batch-size -> 128)

2 		2 hidden layers without BN									64					50			0.95(batch-size -> 128)

3 		2 hudden layers with BN (0.2)								87.5				51			0.3(batch-size -> 128)

4		2 hidden layers with BN (0.5)								67.5				51			1(batch-size -> 128)

5  		2 hidden layers with BN (1)									53					48			1.15(batch-size -> 128)

6		2 hidden layers(500, 40) with BN(0.2)						80					52			0.4 (gotta train properly)

=================================*=======================================*=======================================
7		2 hidden layers (108*108->600->60->6)+BN					75					56.5		0.8 (batch-size -> 64)

8		2 hidden layers (108*108->600->60->6)+BN 					75					56.44		0.8 (batch-size -> 128)

9		2 hidden layers (108*108->1200->120->6)+BN 					82					58.86		0.68 (batch-size -> 32)

10		2 hidden layers (108*108->1200->120->6)+BN 					81.5				57.7		0.73 (batch-size -> 128)

11		2 hidden layers (108*108->1200->120->6)+BN 					84					55.44		0.55 (batch-size -> 128, Less L2 regularization)

12		2 hidden layers (108*108->800->80->6)+BN 					77.5				56.5		0.8 (batch-size -> 128)

13		2 hidden layers (108*108->800->80->6)+BN 					77.5				56.5		0.8 (batch-size -> 128)

14		2 hidden layers (108*108->800->80->6)+BN (only after first)	61.5				51.5		1.09 (batch-size -> 128, Less trained)

15		2 hidden layers (108*108->900->100)							85					55.5		0.5 (batch-size -> 128 less regularized L2 normwise)

16		2 hidden layers (108*108->900->100) 						74.2				58.96		0.796 (batch-size -> 64)

17		2 hidden layers (108*108->900->100) 						72.55				58.86		0.75 (batch-size -> 32)

18		2 hidden layers (108*108->900->100->6) 						65					56.1		1.03 (batch-size -> 32)


      										*******************BEST MODEL*****************
17		2 hidden layers (108*108->900->6) 							72.55				58.86		0.75 (batch-size -> 32)	
											*******************BEST MODEL*****************

19 		3 hidden layers (108*108->900->100->6)						80					56.72		0.85 (batch-size -> 64)


NOTE: In many cases, we accidently left the output dimension as 100 and not only was no error thrown, but there was a surprisingly high accuracy and low loss. In fact these models were amongst the best we trained and gave almost the same accuracy as the models we trained by replacing the 100 by 6. We think this may be because of some kind of regularisation effect because of the remaining weights??

Also, the model takes quite some time to train. To reduce the training time you can replace the "8" in range(8) on line 69 of trainModel.py by 3 as this will give amost the same results in 3/8th the time.